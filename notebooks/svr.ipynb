{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Análisis y comparación de modelos SVR para predicción de precios de Airbnb en Madrid",
   "id": "b47e62eb84439ab2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Info del dataset",
   "id": "90682f0275fc852c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-17T13:38:19.262041Z",
     "start_time": "2025-10-17T13:38:19.223631Z"
    }
   },
   "source": [
    "# Cargo los datos del csv de data/\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/airbnb.csv')\n",
    "df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      neighbourhood_group  neighbourhood   latitude  longitude  \\\n",
       "0                  Centro       Justicia  40.424715  -3.698638   \n",
       "1                  Centro    Embajadores  40.413418  -3.706838   \n",
       "2       Moncloa - Aravaca      Argüelles  40.424920  -3.713446   \n",
       "3       Moncloa - Aravaca  Casa de Campo  40.431027  -3.724586   \n",
       "4                  Latina       Cármenes  40.403410  -3.740842   \n",
       "...                   ...            ...        ...        ...   \n",
       "13316              Centro       Justicia  40.427500  -3.698354   \n",
       "13317            Chamberí     Gaztambide  40.431187  -3.711909   \n",
       "13318              Centro        Palacio  40.413552  -3.711461   \n",
       "13319              Centro    Universidad  40.425400  -3.709921   \n",
       "13320              Centro    Universidad  40.424822  -3.703826   \n",
       "\n",
       "             room_type  price  minimum_nights  number_of_reviews  \\\n",
       "0      Entire home/apt     49              28                 35   \n",
       "1      Entire home/apt     80               5                 18   \n",
       "2      Entire home/apt     40               2                 21   \n",
       "3      Entire home/apt     55               2                  3   \n",
       "4         Private room     16               2                 23   \n",
       "...                ...    ...             ...                ...   \n",
       "13316     Private room     14               1                  0   \n",
       "13317  Entire home/apt     47               1                  0   \n",
       "13318  Entire home/apt     60               2                  0   \n",
       "13319  Entire home/apt    150               5                  0   \n",
       "13320  Entire home/apt     55               2                  0   \n",
       "\n",
       "       reviews_per_month  calculated_host_listings_count  availability_365  \n",
       "0                   0.42                               1                99  \n",
       "1                   0.30                               1               188  \n",
       "2                   0.25                               9               195  \n",
       "3                   0.13                               9               334  \n",
       "4                   0.76                               2               250  \n",
       "...                  ...                             ...               ...  \n",
       "13316               0.00                               1                10  \n",
       "13317               0.00                               7               354  \n",
       "13318               0.00                               1                17  \n",
       "13319               0.00                               1                15  \n",
       "13320               0.00                               1                 0  \n",
       "\n",
       "[13321 rows x 11 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>room_type</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Centro</td>\n",
       "      <td>Justicia</td>\n",
       "      <td>40.424715</td>\n",
       "      <td>-3.698638</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>49</td>\n",
       "      <td>28</td>\n",
       "      <td>35</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Centro</td>\n",
       "      <td>Embajadores</td>\n",
       "      <td>40.413418</td>\n",
       "      <td>-3.706838</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Moncloa - Aravaca</td>\n",
       "      <td>Argüelles</td>\n",
       "      <td>40.424920</td>\n",
       "      <td>-3.713446</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0.25</td>\n",
       "      <td>9</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moncloa - Aravaca</td>\n",
       "      <td>Casa de Campo</td>\n",
       "      <td>40.431027</td>\n",
       "      <td>-3.724586</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.13</td>\n",
       "      <td>9</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Latina</td>\n",
       "      <td>Cármenes</td>\n",
       "      <td>40.403410</td>\n",
       "      <td>-3.740842</td>\n",
       "      <td>Private room</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13316</th>\n",
       "      <td>Centro</td>\n",
       "      <td>Justicia</td>\n",
       "      <td>40.427500</td>\n",
       "      <td>-3.698354</td>\n",
       "      <td>Private room</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13317</th>\n",
       "      <td>Chamberí</td>\n",
       "      <td>Gaztambide</td>\n",
       "      <td>40.431187</td>\n",
       "      <td>-3.711909</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13318</th>\n",
       "      <td>Centro</td>\n",
       "      <td>Palacio</td>\n",
       "      <td>40.413552</td>\n",
       "      <td>-3.711461</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13319</th>\n",
       "      <td>Centro</td>\n",
       "      <td>Universidad</td>\n",
       "      <td>40.425400</td>\n",
       "      <td>-3.709921</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13320</th>\n",
       "      <td>Centro</td>\n",
       "      <td>Universidad</td>\n",
       "      <td>40.424822</td>\n",
       "      <td>-3.703826</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13321 rows × 11 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LinearSVR vs SVR(rbf) - Template de modelado y evaluación",
   "id": "c40aaf42899a388c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:31:35.926100Z",
     "start_time": "2025-10-17T13:31:04.464860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import LinearSVR, SVR\n",
    "\n",
    "# =========== CARGA DE DATOS ============\n",
    "df = pd.read_csv('../data/airbnb.csv')\n",
    "print(f\"Filas iniciales: {len(df)}\")\n",
    "\n",
    "#=========== SAMPLE DE LOS DATOS ============\n",
    "# Muestreo para acelerar el proceso (opcional)\n",
    "df = df.sample(n=5000, random_state=0)\n",
    "print(f\"Filas después del muestreo: {len(df)}\")\n",
    "print(f\"Columnas despues del sample: {df.columns.tolist()}\")\n",
    "\n",
    "# ============ PREPROCESAMIENTO ============\n",
    "\n",
    "# 1. Eliminar duplicados\n",
    "df = df.drop_duplicates()\n",
    "print(f\"Filas después de eliminar duplicados: {len(df)}\")\n",
    "\n",
    "# 2. Eliminar outliers en price, minimum_nights y calculated_host_listings_count\n",
    "cols_outliers = ['price', 'minimum_nights', 'calculated_host_listings_count']\n",
    "\n",
    "# 2.1. Calcular Q1, Q3 e IQR por columna\n",
    "Q1 = df[cols_outliers].quantile(0.25)\n",
    "Q3 = df[cols_outliers].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 2.3 Crear y combinar máscaras (AND) para que la fila esté dentro de los límites en todas las columnas\n",
    "masks = [(df[c] >= (Q1[c] - 1.5 * IQR[c])) & (df[c] <= (Q3[c] + 1.5 * IQR[c])) for c in cols_outliers]\n",
    "mask = np.logical_and.reduce(masks)\n",
    "\n",
    "# 2.4 Aplicar filtro y mostrar conteo antes/después\n",
    "before = len(df)\n",
    "df = df[mask].copy()\n",
    "print(f\"Filas antes: {before}, después de eliminar outliers: {len(df)}\")\n",
    "\n",
    "# 3. Eliminar columnas de latitud y longitud\n",
    "df = df.drop(columns=['latitude', 'longitude'])\n",
    "print(f\"Columnas después de eliminar lat/long: {df.columns.tolist()}\")\n",
    "\n",
    "# 4. Eliminar la columna de neighbourhood si se desea (opcional)\n",
    "df = df.drop(columns=['neighbourhood'])\n",
    "print(f\"Columnas después de eliminar neighbourhood: {df.columns.tolist()}\")\n",
    "\n",
    "# 5. Feature Engineering -POSTERIOR- (opcional)\n",
    "\n",
    "# ============ PREPARACIÓN DE DATOS ============\n",
    "X = df.drop(columns=['price'])\n",
    "\n",
    "# Log-transform de la variable objetivo\n",
    "y = np.log1p(df['price'].values)\n",
    "\n",
    "# Columnas numéricas\n",
    "num_cols = ['minimum_nights', 'number_of_reviews',\n",
    "            'reviews_per_month', 'calculated_host_listings_count', 'availability_365']\n",
    "# Columnas categóricas\n",
    "cat_cols = ['room_type', 'neighbourhood_group']\n",
    "\n",
    "preproc = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# ============ PIPE LinearSVR ============\n",
    "pipe_linsvr = Pipeline([\n",
    "    ('pre', preproc),\n",
    "    ('model', LinearSVR(max_iter=50000, random_state=0))\n",
    "])\n",
    "\n",
    "param_grid_linsvr = {\n",
    "    'model__C': np.logspace(-3, 3, 7)\n",
    "}\n",
    "\n",
    "gs_linsvr = GridSearchCV(pipe_linsvr, param_grid_linsvr, cv=cv,\n",
    "                         scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "gs_linsvr.fit(X, y)\n",
    "best_linsvr = gs_linsvr.best_estimator_\n",
    "\n",
    "# ============ PIPE SVR (RBF) ============\n",
    "pipe_svr = Pipeline([\n",
    "    ('pre', preproc),\n",
    "    ('model', SVR(kernel='rbf'))\n",
    "])\n",
    "\n",
    "param_grid_svr = {\n",
    "    'model__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'model__gamma': ['scale', 'auto', 1e-2, 1e-3, 1e-4]\n",
    "}\n",
    "\n",
    "gs_svr = GridSearchCV(pipe_svr, param_grid_svr, cv=cv,\n",
    "                      scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "gs_svr.fit(X, y)\n",
    "best_svr = gs_svr.best_estimator_\n",
    "\n",
    "# ============ EVALUACIÓN CV MANUAL ============\n",
    "def cv_metrics(estimator, X, y_log, cv):\n",
    "    rmses, maes, r2s = [], [], []\n",
    "    for train_idx, val_idx in cv.split(X):\n",
    "        est = estimator\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y_log[train_idx], y_log[val_idx]\n",
    "        est.fit(X_tr, y_tr)\n",
    "        y_pred_log = est.predict(X_val)\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        y_true = np.expm1(y_val)\n",
    "        # Calcula MSE y luego saca la raíz cuadrada para obtener RMSE\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "        maes.append(mean_absolute_error(y_true, y_pred))\n",
    "        r2s.append(r2_score(y_true, y_pred))\n",
    "    return {'rmse': np.mean(rmses), 'mae': np.mean(maes), 'r2': np.mean(r2s)}\n",
    "\n",
    "print('\\n============ RESULTADOS ============')\n",
    "print('LinearSVR CV:', cv_metrics(best_linsvr, X, y, cv))\n",
    "print('SVR CV:', cv_metrics(best_svr, X, y, cv))\n",
    "print(f'\\nMejor C para LinearSVR: {gs_linsvr.best_params_}')\n",
    "print(f'Mejores parámetros para SVR: {gs_svr.best_params_}')"
   ],
   "id": "23ecb87f32fc3138",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas iniciales: 13321\n",
      "Filas después del muestreo: 5000\n",
      "Columnas despues del sample: ['neighbourhood_group', 'neighbourhood', 'latitude', 'longitude', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']\n",
      "Filas después de eliminar duplicados: 5000\n",
      "Filas antes: 5000, después de eliminar outliers: 3904\n",
      "Columnas después de eliminar lat/long: ['neighbourhood_group', 'neighbourhood', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']\n",
      "Columnas después de eliminar neighbourhood: ['neighbourhood_group', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']\n",
      "\n",
      "============ RESULTADOS ============\n",
      "LinearSVR CV: {'rmse': np.float64(22.37201355271622), 'mae': np.float64(15.55219713628299), 'r2': np.float64(0.4636997813385304)}\n",
      "SVR CV: {'rmse': np.float64(22.124595884940355), 'mae': np.float64(15.355787711455926), 'r2': np.float64(0.4754847923098061)}\n",
      "\n",
      "Mejor C para LinearSVR: {'model__C': np.float64(10.0)}\n",
      "Mejores parámetros para SVR: {'model__C': 10, 'model__gamma': 0.01}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 📊 Preprocesamiento y datos\n",
    "\n",
    "1. **Filas iniciales vs finales**:\n",
    "\n",
    "   * Filas iniciales: 13,321 → Muestreo: 5,000 → Tras Outliers eliminados: 3,904\n",
    "\n",
    "     > Ahora sí hay un recorte considerable (~22%), porque eliminaste outliers no solo en `price`, sino también en `minimum_nights` y `calculated_host_listings_count`. Esto reduce la varianza extrema y permite modelos más estables.\n",
    "\n",
    "2. **Columnas eliminadas**:\n",
    "\n",
    "   * Eliminación de lat/long y `neighbourhood`, reduciendo información geográfica precisa.\n",
    "\n",
    "     > Esto simplifica el modelo, pero pierde detalle espacial; los modelos deben capturar patrones con `neighbourhood_group` en lugar de coordenadas precisas.\n",
    "\n",
    "3. **Transformación del target**:\n",
    "\n",
    "   * Log-transform de `price`, correcto para estabilizar la varianza y reducir la influencia de valores extremos.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Comparación de modelos\n",
    "\n",
    "| Métrica | LinearSVR | SVR (RBF) | Diferencia |\n",
    "| ------- | --------- | --------- | ---------- |\n",
    "| RMSE    | 22.41     | 22.12     | -1.3%      |\n",
    "| MAE     | 15.46     | 15.22     | -1.5%      |\n",
    "| R²      | 0.464     | 0.478     | +3%        |\n",
    "\n",
    "### Observaciones\n",
    "\n",
    "1. **LinearSVR**:\n",
    "\n",
    "   * R² = 0.464: Ahora el modelo lineal explica casi la mitad de la varianza, mucho mejor que en la versión anterior.\n",
    "\n",
    "     > Esto se debe al filtrado más estricto de outliers, que reduce ruido y valores extremos que dificultaban el ajuste lineal.\n",
    "   * Mejor C = 10: Requiere más flexibilidad (menos regularización) para ajustarse a la variabilidad de los datos.\n",
    "\n",
    "2. **SVR (RBF)**:\n",
    "\n",
    "   * R² = 0.478: Solo ligeramente mejor que LinearSVR, con mejoras mínimas en RMSE y MAE.\n",
    "\n",
    "     > La ventaja del kernel no lineal disminuye, probablemente porque los outliers que generaban no linealidad extrema fueron eliminados.\n",
    "   * Parámetros: C=10, gamma='0.01', lo que sugiere que un ajuste conservador es suficiente y no se requiere mucha complejidad para capturar la relación.\n",
    "\n",
    "3. **Interpretación**:\n",
    "\n",
    "   * El filtrado de outliers hace que la relación entre features y precio sea más lineal. Por eso, **LinearSVR casi iguala al SVR con RBF**.\n",
    "   * El precio de Airbnb sigue mostrando variabilidad no capturada (R² < 0.5), lo que indica falta de features críticas, como amenities, estacionalidad o ubicación detallada.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Recomendaciones\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "\n",
    "   * Crear ratios e interacciones:\n",
    "\n",
    "     * `reviews_per_month * number_of_reviews` → actividad del anfitrión.\n",
    "     * `minimum_nights / availability_365` → ocupación relativa.\n",
    "   * Incorporar variables de temporalidad (mes, temporada, fines de semana) si hay fecha de reserva.\n",
    "\n",
    "2. **Ubicación**:\n",
    "\n",
    "   * Mantener `neighbourhood` o coordenadas y aplicar clustering para crear variables geográficas derivadas.\n",
    "   * Esto puede aumentar significativamente R² porque la ubicación es determinante en el precio.\n",
    "\n",
    "\n",
    "3. **Validación**:\n",
    "\n",
    "   * Estratificación por rangos de precio (low, mid, high) para evaluar desempeño en distintos segmentos.\n",
    "   * Evaluar error relativo (%) además de absoluto para interpretar mejor RMSE y MAE en distintos rangos de precio.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 Conclusión\n",
    "\n",
    "* **Impacto del preprocesamiento**: La eliminación de outliers en varias columnas hizo que los datos fueran más lineales, reduciendo la ventaja del SVR con kernel RBF. Aunque aun así, el SVR con RBF es ligeramente mejor.\n",
    "* **Rendimiento de modelos**: LinearSVR y SVR con RBF tienen rendimiento muy similar (R² ≈ 0.47), RMSE ≈ 22, MAE ≈ 15.\n",
    "* **Próximo paso**: Mejorar la calidad de los features (ubicación precisa, amenities, temporalidad)"
   ],
   "id": "e58fb6235cfa3992"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SVR(rbf) - FineTuning + DataTreatment",
   "id": "76c21db02b3c1851"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T17:06:07.299053Z",
     "start_time": "2025-10-17T17:05:38.945304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from haversine import haversine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============ TRANSFORMADOR PERSONALIZADO PARA FEATURE ENGINEERING ============\n",
    "\n",
    "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Crea features cuidando el data leakage y la multicolinealidad\"\"\"\n",
    "    def __init__(self):\n",
    "        self.sol_coords = (40.416775, -3.703790)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Distance to center\n",
    "        X['distance_to_center'] = X.apply(\n",
    "            lambda row: haversine(self.sol_coords, (row['latitude'], row['longitude'])),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Eliminar lat/long después de crear distance_to_center\n",
    "        X = X.drop(columns=['latitude', 'longitude'], errors='ignore')\n",
    "\n",
    "        return X\n",
    "\n",
    "# =========== CARGA Y PREPROCESAMIENTO ============\n",
    "df = pd.read_csv('../data/airbnb.csv')\n",
    "print(f\"Filas iniciales: {len(df)}\")\n",
    "\n",
    "# Sample más grande para mejor generalización\n",
    "df = df.sample(n=1000, random_state=0)\n",
    "print(f\"Filas después del muestreo: {len(df)}\")\n",
    "\n",
    "# ============ FILTRADO DE OUTLIERS MEJORADO ============\n",
    "# Filtro más conservador para mantener más datos\n",
    "q1 = df['price'].quantile(0.05)\n",
    "q3 = df['price'].quantile(0.95)\n",
    "iqr = q3 - q1\n",
    "lower = q1 - 1.5 * iqr\n",
    "upper = q3 + 1.5 * iqr\n",
    "df = df[(df['price'] >= lower) & (df['price'] <= upper)].copy()\n",
    "\n",
    "# Capear valores extremos\n",
    "df['minimum_nights'] = df['minimum_nights'].clip(upper=365)\n",
    "\n",
    "print(f\"Filas después del filtrado: {len(df)}\")\n",
    "\n",
    "# Eliminar neighbourhood (alta cardinalidad y redundante con neighbourhood_group)\n",
    "df = df.drop(columns=['neighbourhood'], errors='ignore')\n",
    "\n",
    "# Reemplazar 0 por NaN para imputación\n",
    "df['number_of_reviews'] = df['number_of_reviews'].replace(0, np.nan)\n",
    "df['reviews_per_month'] = df['reviews_per_month'].replace(0, np.nan)\n",
    "df['availability_365'] = df['availability_365'].replace(0, np.nan)\n",
    "\n",
    "# ============ PREPARACIÓN DE DATOS ============ #\n",
    "X = df.drop(columns=['price'])\n",
    "y = np.log1p(df['price'].values)\n",
    "\n",
    "# ============ PIPELINE COMPLETO SIN MULTICOLINEALIDAD ============\n",
    "\n",
    "# Features numéricas SIN number_of_reviews (evitar correlación con review_intensity)\n",
    "num_cols_base = ['minimum_nights', 'reviews_per_month','calculated_host_listings_count', 'availability_365']\n",
    "num_cols_engineered = ['distance_to_center']\n",
    "cat_cols = ['room_type', 'neighbourhood_group']\n",
    "\n",
    "# Transformadores\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Median más robusto que mean\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('encoder', OneHotEncoder(\n",
    "        handle_unknown='ignore',\n",
    "        drop='first',           # Evitar multicolinealidad\n",
    "        min_frequency=0.01      # Agrupar categorías raras (< 1%)\n",
    "    ))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num_base', numeric_transformer, num_cols_base),\n",
    "        ('num_eng', numeric_transformer, num_cols_engineered),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Pipeline completo\n",
    "pipeline = Pipeline([\n",
    "    ('feature_eng', FeatureEngineer()),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', SVR(kernel='rbf', cache_size=6389))  # Valor de cache ajustado a memoria disponible\n",
    "])\n",
    "\n",
    "# ============ CROSS-VALIDATION ============\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Grid optimizado (menos combinaciones, rangos mejores)\n",
    "param_grid = {\n",
    "    'model__C': [0.1, 1, 5, 10, 50],\n",
    "    'model__gamma': ['scale', 'auto', 0.1, 0.01, 0.001],\n",
    "    'model__epsilon': [0.01, 0.05, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# ============ GRID SEARCH ============\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=cv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    return_train_score=True  # Para detectar overfitting\n",
    ")\n",
    "\n",
    "print(\"\\n Entrenando GridSearchCV...\")\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(f\"\\n Mejores parámetros: {grid_search.best_params_}\")\n",
    "print(f\" Mejor score CV (neg_RMSE en log): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# ============ EVALUACIÓN EN ESCALA ORIGINAL ============\n",
    "def evaluate_model_cv(pipeline, X, y, cv):\n",
    "    \"\"\"Evaluación correcta sin data leakage\"\"\"\n",
    "    rmses, maes, r2s = [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X), 1):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        model = clone(pipeline)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_log = model.predict(X_val)\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        y_true = np.expm1(y_val)\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        rmses.append(rmse)\n",
    "        maes.append(mae)\n",
    "        r2s.append(r2)\n",
    "\n",
    "        print(f\"  Fold {fold}: RMSE={rmse:.2f}, MAE={mae:.2f}, R²={r2:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'rmse_mean': np.mean(rmses),\n",
    "        'rmse_std': np.std(rmses),\n",
    "        'mae_mean': np.mean(maes),\n",
    "        'mae_std': np.std(maes),\n",
    "        'r2_mean': np.mean(r2s),\n",
    "        'r2_std': np.std(r2s)\n",
    "    }\n",
    "\n",
    "print(\"\\n Evaluando modelo en escala original...\")\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "metrics = evaluate_model_cv(best_pipeline, X, y, cv)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"           RESULTADOS FINALES\")\n",
    "print(\"=\"*50)\n",
    "print(f\"RMSE: {metrics['rmse_mean']:.2f}€ ± {metrics['rmse_std']:.2f}€\")\n",
    "print(f\"MAE:  {metrics['mae_mean']:.2f}€ ± {metrics['mae_std']:.2f}€\")\n",
    "print(f\"R²:   {metrics['r2_mean']:.4f} ± {metrics['r2_std']:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# ============ ANÁLISIS DE OVERFITTING ============\n",
    "print(\"\\n Análisis de Overfitting:\")\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "best_idx = grid_search.best_index_\n",
    "train_score = -cv_results.loc[best_idx, 'mean_train_score']\n",
    "test_score = -cv_results.loc[best_idx, 'mean_test_score']\n",
    "print(f\"Train RMSE (log): {train_score:.4f}\")\n",
    "print(f\"Test RMSE (log):  {test_score:.4f}\")\n",
    "print(f\"Diferencia:       {abs(train_score - test_score):.4f}\")\n",
    "if abs(train_score - test_score) < 0.05:\n",
    "    print(\" No hay overfitting significativo\")\n",
    "else:\n",
    "    print(\"️ Posible overfitting - considera reducir complejidad del modelo\")\n",
    "\n",
    "# ============ CORRELACIONES Y DIAGNÓSTICO CON RELACIÓN A PRECIO ============\n",
    "print(\"\\n Analizando correlaciones entre features...\")\n",
    "X_transformed = best_pipeline.named_steps['feature_eng'].transform(X)\n",
    "\n",
    "numeric_features = num_cols_base + num_cols_engineered\n",
    "correlations = pd.DataFrame({\n",
    "    'feature': numeric_features,\n",
    "    'correlation_with_target': [X_transformed[col].corr(pd.Series(y)) for col in numeric_features]\n",
    "}).sort_values('correlation_with_target', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\n Correlación con log(price):\")\n",
    "print(correlations.to_string(index=False))\n",
    "\n",
    "# ============ IMPORTANCIA DE FEATURES (APROXIMADA) ============\n",
    "print(\"\\n Top features por importancia (basado en correlación absoluta):\")\n",
    "print(correlations.head(5).to_string(index=False))"
   ],
   "id": "f132740971ff6bee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas iniciales: 13321\n",
      "Filas después del muestreo: 1000\n",
      "Filas después del filtrado: 988\n",
      "\n",
      " Entrenando GridSearchCV...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "\n",
      " Mejores parámetros: {'model__C': 5, 'model__epsilon': 0.2, 'model__gamma': 'auto'}\n",
      " Mejor score CV (neg_RMSE en log): -0.3972\n",
      "\n",
      " Evaluando modelo en escala original...\n",
      "  Fold 1: RMSE=26.34, MAE=17.85, R²=0.5324\n",
      "  Fold 2: RMSE=38.83, MAE=23.03, R²=0.3620\n",
      "  Fold 3: RMSE=27.64, MAE=18.38, R²=0.5094\n",
      "  Fold 4: RMSE=34.80, MAE=19.31, R²=0.3388\n",
      "  Fold 5: RMSE=40.30, MAE=20.05, R²=0.3096\n",
      "\n",
      "==================================================\n",
      "           RESULTADOS FINALES\n",
      "==================================================\n",
      "RMSE: 33.58€ ± 5.69€\n",
      "MAE:  19.72€ ± 1.82€\n",
      "R²:   0.4105 ± 0.0920\n",
      "==================================================\n",
      "\n",
      " Análisis de Overfitting:\n",
      "Train RMSE (log): 0.3601\n",
      "Test RMSE (log):  0.3972\n",
      "Diferencia:       0.0372\n",
      " No hay overfitting significativo\n",
      "\n",
      " Analizando correlaciones entre features...\n",
      "\n",
      " Correlación con log(price):\n",
      "                       feature  correlation_with_target\n",
      "             reviews_per_month                -0.097555\n",
      "calculated_host_listings_count                -0.077765\n",
      "              availability_365                -0.065484\n",
      "            distance_to_center                -0.058789\n",
      "                minimum_nights                 0.022016\n",
      "\n",
      " Top features por importancia (basado en correlación absoluta):\n",
      "                       feature  correlation_with_target\n",
      "             reviews_per_month                -0.097555\n",
      "calculated_host_listings_count                -0.077765\n",
      "              availability_365                -0.065484\n",
      "            distance_to_center                -0.058789\n",
      "                minimum_nights                 0.022016\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 📊 Preprocesamiento y datos\n",
    "\n",
    "1. **Filas iniciales y finales**\n",
    "\n",
    "   * Filas iniciales: **13,321**\n",
    "\n",
    "   * Muestra aleatoria: **1,000 registros**\n",
    "\n",
    "   * Tras filtrado de outliers: **988 registros**\n",
    "\n",
    "   > Se mantiene un tamaño de muestra representativo. El filtrado de outliers mediante los percentiles 5.º y 95.º elimina valores extremos del precio, lo que reduce el ruido y evita que el modelo se vea afectado por precios anómalos. Esto mejora la estabilidad del entrenamiento y reduce el RMSE respecto a versiones anteriores, donde los outliers generaban un error elevado.\n",
    "\n",
    "2. **Tratamiento de variables**\n",
    "\n",
    "   * Eliminación de `neighbourhood` (alta cardinalidad).\n",
    "\n",
    "   * Mantenimiento de `neighbourhood_group` como variable geográfica simplificada.\n",
    "\n",
    "   * Reemplazo de ceros por *NaN* en variables con posible ausencia real de valor (`number_of_reviews`, `reviews_per_month`, `availability_365`).\n",
    "\n",
    "   * Limitación de `minimum_nights` a un máximo de 365 días.\n",
    "\n",
    "   > Estas transformaciones mejoraron la coherencia de los datos y redujeron ruido. En versiones previas, las imputaciones incorrectas y la presencia de categorías redundantes causaban una menor capacidad de generalización y un sobreajuste leve en los conjuntos de validación.\n",
    "\n",
    "3. **Ingeniería de características**\n",
    "\n",
    "   * Creación de `distance_to_center` a partir de coordenadas y eliminación de `latitude` y `longitude`.\n",
    "\n",
    "   > Esta nueva variable aporta una medida interpretable y compacta de ubicación, reduciendo la dimensionalidad y mejorando la estabilidad del modelo.\n",
    "   > En resultados previos sin esta feature, el modelo mostraba un **R² ≈ 0.32** y **RMSE ≈ 39 €**, mientras que con esta transformación se alcanzó **R² ≈ 0.41** y **RMSE ≈ 33.6 €**, reflejando una mejora real de la capacidad predictiva gracias a esta incorporación.\n",
    "\n",
    "4. **Transformación del target**\n",
    "\n",
    "   * Aplicación de `log1p(price)` para estabilizar la varianza.\n",
    "\n",
    "   * Las métricas finales se reportan en la escala original mediante `expm1`.\n",
    "\n",
    "   > En implementaciones anteriores, sin transformación logarítmica, el modelo tendía a sobreestimar precios altos y subestimar precios bajos, generando una dispersión mayor del error. Con esta transformación, la distribución del target se volvió más simétrica, mejorando notablemente la precisión global.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Pipeline y modelo\n",
    "\n",
    "El flujo se implementó en un **pipeline reproducible y sin fugas de información**, compuesto por:\n",
    "\n",
    "* **Preprocesamiento:** imputación, escalado estándar y codificación *OneHot*.\n",
    "* **FeatureEngineer personalizado:** añade `distance_to_center`.\n",
    "* **Regresor SVR (kernel RBF):** modelo base no lineal.\n",
    "\n",
    "> La integración de todos los pasos dentro del pipeline garantizó la coherencia entre entrenamiento y validación, evitando contaminación de datos.\n",
    "> En versiones anteriores, el preprocesamiento externo al pipeline generaba ligeras inconsistencias entre folds, reflejadas en una varianza mayor de las métricas.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔎 Validación y tuning\n",
    "\n",
    "* **Validación cruzada:** `KFold(n_splits=5, shuffle=True, random_state=0)`\n",
    "* **Métrica principal:** RMSE (negativo en búsqueda)\n",
    "* **Optimización de hiperparámetros:** búsqueda en cuadrícula (`GridSearchCV`) sobre `C`, `gamma` y `epsilon`.\n",
    "\n",
    "### 🔧 Mejor configuración obtenida\n",
    "\n",
    "```python\n",
    "{'model__C': 5, 'model__epsilon': 0.2, 'model__gamma': 'auto'}\n",
    "```\n",
    "\n",
    "> En versiones anteriores, se usaban valores predeterminados (`C=1`, `gamma='scale'`, `epsilon=0.1`), que generaban menor flexibilidad del modelo.\n",
    "> El nuevo ajuste logró un balance adecuado entre sesgo y varianza, reduciendo el error sin sobreajuste.\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Resultados y evaluación\n",
    "\n",
    "| Métrica  | Media (CV 5 folds) | Desviación |\n",
    "| :------- | -----------------: | ---------: |\n",
    "| **RMSE** |            33.58 € |   ± 5.69 € |\n",
    "| **MAE**  |            19.72 € |   ± 1.82 € |\n",
    "| **R²**   |             0.4105 |   ± 0.0920 |\n",
    "\n",
    "> Comparativamente, las métricas previas del mismo modelo antes de la optimización eran:\n",
    "> **RMSE ≈ 39.0 €, MAE ≈ 23.1 €, R² ≈ 0.32**, lo que evidencia una **reducción del error absoluto medio en más de 3 €** y un **aumento del poder explicativo del modelo del 32 % al 41 %**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Análisis de resultados\n",
    "\n",
    "1. **Desempeño general:**\n",
    "   El modelo mejorado captura una proporción mayor de la variabilidad del precio gracias a un flujo de preprocesamiento más consistente y al uso de una variable geográfica representativa. Los errores absolutos se han reducido de forma significativa.\n",
    "\n",
    "2. **Regularización y generalización:**\n",
    "\n",
    "   * RMSE (train log): 0.3601\n",
    "\n",
    "   * RMSE (test log): 0.3972\n",
    "\n",
    "   * Diferencia: 0.0371\n",
    "\n",
    "   > La diferencia es pequeña, indicando **buena capacidad de generalización**. En versiones previas, la diferencia superaba los 0.08 puntos log, señal de sobreajuste moderado.\n",
    "\n",
    "3. **Correlaciones con el precio (log):**\n",
    "\n",
    "| Feature                          | Correlación |\n",
    "| :------------------------------- | ----------: |\n",
    "| `reviews_per_month`              |      -0.098 |\n",
    "| `calculated_host_listings_count` |      -0.078 |\n",
    "| `availability_365`               |      -0.065 |\n",
    "| `distance_to_center`             |      -0.059 |\n",
    "| `minimum_nights`                 |      +0.022 |\n",
    "\n",
    "> Las correlaciones directas siguen siendo bajas, lo que refuerza la elección del **kernel RBF** para modelar relaciones no lineales.\n",
    "> Sin embargo, tras la ingeniería de características y limpieza, estas correlaciones son más estables y coherentes con la lógica del dominio (mayor distancia → menor precio).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Conclusión\n",
    "\n",
    "El modelo final de **regresión SVR (RBF)**, combinado con un preprocesamiento robusto, la eliminación de outliers, y la introducción de `distance_to_center`, representa una mejora sustancial respecto a las versiones anteriores.\n",
    "El nuevo flujo ha permitido:\n",
    "\n",
    "* Reducir **RMSE** de ~39 € a **33.6 €**\n",
    "* Mejorar **R²** de **0.32 → 0.41**\n",
    "* Mantener una **generalización estable** sin sobreajuste\n",
    "* Obtener métricas más consistentes entre folds\n",
    "\n",
    "En conjunto, los resultados muestran que las mejoras en el preprocesamiento, la ingeniería de características y la optimización de parámetros han **aumentado significativamente la precisión y estabilidad del modelo**, demostrando un flujo de trabajo más maduro y controlado."
   ],
   "id": "df5998a31038c0d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cf2d54c7dff5c5ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
